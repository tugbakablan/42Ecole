{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam or Ham (project 1)\n",
    "\n",
    "## 1- Introduction with the Bernoulli model\n",
    "\n",
    "\n",
    "We say that a random variables $X \\in \\{0, 1\\}$ follows a Bernoulli distribution of parameter $\\theta$ if $\\mathbb{P}(X = 1) = \\theta$ and $\\mathbb{P}(X = 0) = 1 âˆ’ \\theta$.\n",
    "\n",
    "\n",
    "1.  Show that we can write the probability distribution of $X$ in a compact form as : \n",
    "$$\n",
    "\\mathbb{P}(X = x) = \\theta^{x} (1 âˆ’ \\theta)^{1âˆ’x}\n",
    "$$\n",
    "\n",
    "2. Suppose now that we have a set of n independent variables $x_1,...,x_n$. If we note $n_1 = \\sum_{i=1}^n \\mathbb{1}_{\\{x_i=1\\}}$ and $n_0 = n âˆ’ n_1$, show that :\n",
    "$$\n",
    "\\mathbb{P}(x_1,\\ldots,x_n \\mid \\theta)=\\theta^{n_1} (1âˆ’\\theta)^{n_0}\n",
    "$$\n",
    "\n",
    "3. Show that the maximum likelihood estimator is $\\hat{\\theta}_{ML} = \\frac{n_1}{n}$\n",
    "\n",
    "4. A conjugate prior for the Bernoulli distribution is the Beta distribution. \n",
    "$$\n",
    "Beta(\\theta \\mid a, b) \\propto \\theta^{aâˆ’1} (1 âˆ’ \\theta)^{bâˆ’1}\n",
    "$$\n",
    "The Beta distribution has the following properties for its expectation and mode (for more details you can look in one of the books like the Bishop):\n",
    "$$\n",
    "\\mathbb{E}(\\theta) = \\frac{a}{a+b} \\text{, mode}(\\theta) = \\frac{a-1}{a+b-2}\n",
    "$$\n",
    "\n",
    "Show that with a $Beta(a,b)$ prior the posterior distribution \n",
    "$\\mathbb{P}(\\theta \\mid x_1,\\ldots, x_n)$\n",
    "is proportional to $\\theta^{n_1+a-1} \\cdot (1 âˆ’ \\theta)^{n_0+b-1}$ \n",
    "\n",
    "5. (Those two questions are optional, you can also simply use the result in the following)\n",
    "\n",
    "  a. Show that the maximum a posteriori _mode_ estimate is in the form $\\bar{\\theta}_{MAP} = \\frac{n_1+a-1}{n+a+b-2}$\n",
    "\n",
    "  b. Show that the maximum a posteriori \n",
    "_mean_ estimate is in the form $\\hat{\\theta}_{MAP} = \\mathbb{E}(\\theta \\mid x_1,\\ldots, x_n) = \n",
    "\\int_{\\theta =0}^1 \\mathbb{P}(\\theta \\mid x_1\\ldots x_n) d\\theta  = \\frac{n_1+a}{n+a+b}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compact Form of the Bernoulli Distribution\n",
    "\n",
    "A random variable \\( X \\in \\{0, 1\\} \\) follows a Bernoulli distribution with parameter \\( \\theta \\in [0, 1] \\) if:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X = 1) = \\theta \\quad \\text{and} \\quad \\mathbb{P}(X = 0) = 1 - \\theta\n",
    "\\]\n",
    "\n",
    "Let's show that this can be written in the following compact form:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X = x) = \\theta^x (1 - \\theta)^{1 - x}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Case 1: \\( x = 1 \\)\n",
    "\n",
    "If \\( x = 1 \\), we can calculate:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X = 1) = \\theta^1 (1 - \\theta)^{1 - 1} = \\theta \\cdot 1 = \\theta\n",
    "\\]\n",
    "\n",
    "âœ… This matches the definition.\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Case 2: \\( x = 0 \\)\n",
    "\n",
    "If \\( x = 0 \\), we get:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X = 0) = \\theta^0 (1 - \\theta)^{1 - 0} = 1 \\cdot (1 - \\theta) = 1 - \\theta\n",
    "\\]\n",
    "\n",
    "âœ… This also matches the definition.\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Conclusion:\n",
    "\n",
    "The expression\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(X = x) = \\theta^x (1 - \\theta)^{1 - x}\n",
    "\\]\n",
    "\n",
    "correctly describes both cases (\\( x = 0 \\) and \\( x = 1 \\)) and thus represents a **compact and general form** of the Bernoulli distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Likelihood of a Set of Independent Bernoulli Trials\n",
    "\n",
    "Suppose we have a dataset consisting of \\( n \\) independent observations:\n",
    "\n",
    "\\[\n",
    "x_1, x_2, \\ldots, x_n \\quad \\text{where each} \\quad x_i \\in \\{0, 1\\}\n",
    "\\]\n",
    "\n",
    "Each \\( x_i \\) is assumed to follow a Bernoulli distribution with the same parameter \\( \\theta \\). Since the observations are independent, the joint probability (likelihood) of the dataset is the product of the individual probabilities:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(x_1, x_2, \\ldots, x_n \\mid \\theta) = \\prod_{i=1}^{n} \\mathbb{P}(x_i \\mid \\theta)\n",
    "\\]\n",
    "\n",
    "From **Question 1**, we know:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(x_i \\mid \\theta) = \\theta^{x_i} (1 - \\theta)^{1 - x_i}\n",
    "\\]\n",
    "\n",
    "So we can write:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(x_1, \\ldots, x_n \\mid \\theta) = \\prod_{i=1}^{n} \\theta^{x_i} (1 - \\theta)^{1 - x_i}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Combine the powers:\n",
    "\n",
    "Let:\n",
    "- \\( n_1 = \\sum_{i=1}^{n} \\mathbb{1}_{\\{x_i = 1\\}} \\): the number of 1's in the dataset\n",
    "- \\( n_0 = n - n_1 \\): the number of 0's\n",
    "\n",
    "Then:\n",
    "\n",
    "\\[\n",
    "\\prod_{i=1}^{n} \\theta^{x_i} (1 - \\theta)^{1 - x_i} = \\theta^{n_1} (1 - \\theta)^{n_0}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Conclusion:\n",
    "\n",
    "The likelihood of the dataset is:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(x_1, \\ldots, x_n \\mid \\theta) = \\theta^{n_1} (1 - \\theta)^{n_0}\n",
    "\\]\n",
    "\n",
    "This compact expression summarizes the contribution of all 1's and 0's in the dataset to the likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Maximum Likelihood Estimation (MLE) of \\( \\theta \\)\n",
    "\n",
    "We are given a dataset \\( x_1, x_2, \\ldots, x_n \\), where each \\( x_i \\in \\{0, 1\\} \\) follows a Bernoulli distribution with parameter \\( \\theta \\). \n",
    "\n",
    "From **Question 2**, the likelihood function is:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(x_1, \\ldots, x_n \\mid \\theta) = \\theta^{n_1} (1 - \\theta)^{n_0}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( n_1 = \\sum_{i=1}^{n} x_i \\)\n",
    "- \\( n_0 = n - n_1 \\)\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step 1: Log-Likelihood\n",
    "\n",
    "We take the natural logarithm of the likelihood function to obtain the log-likelihood:\n",
    "\n",
    "\\[\n",
    "\\log \\mathcal{L}(\\theta) = \\log \\left( \\theta^{n_1} (1 - \\theta)^{n_0} \\right)\n",
    "= n_1 \\log \\theta + n_0 \\log (1 - \\theta)\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Step 2: Differentiate and Maximize\n",
    "\n",
    "To find the MLE, we take the derivative of the log-likelihood with respect to \\( \\theta \\):\n",
    "\n",
    "\\[\n",
    "\\frac{d}{d\\theta} \\log \\mathcal{L}(\\theta) = \\frac{n_1}{\\theta} - \\frac{n_0}{1 - \\theta}\n",
    "\\]\n",
    "\n",
    "Set the derivative to zero to find the maximum:\n",
    "\n",
    "\\[\n",
    "\\frac{n_1}{\\theta} - \\frac{n_0}{1 - \\theta} = 0\n",
    "\\]\n",
    "\n",
    "Multiply both sides by \\( \\theta(1 - \\theta) \\):\n",
    "\n",
    "\\[\n",
    "n_1 (1 - \\theta) = n_0 \\theta\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "n_1 - n_1 \\theta = n_0 \\theta\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "n_1 = (n_0 + n_1) \\theta = n \\theta\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Maximum Likelihood Estimator:\n",
    "\n",
    "\\[\n",
    "\\hat{\\theta}_{ML} = \\frac{n_1}{n}\n",
    "\\]\n",
    "\n",
    "This is simply the proportion of 1's in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Posterior Distribution with Beta Prior (Conjugacy)\n",
    "\n",
    "We are using a **Beta prior** for the Bernoulli distribution parameter \\( \\theta \\):\n",
    "\n",
    "\\[\n",
    "\\text{Prior: } \\mathbb{P}(\\theta) = \\text{Beta}(\\theta \\mid a, b) \\propto \\theta^{a - 1}(1 - \\theta)^{b - 1}\n",
    "\\]\n",
    "\n",
    "From **Question 2**, the likelihood of the data \\( x_1, \\ldots, x_n \\) is:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(x_1, \\ldots, x_n \\mid \\theta) = \\theta^{n_1}(1 - \\theta)^{n_0}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( n_1 = \\sum_{i=1}^{n} x_i \\)\n",
    "- \\( n_0 = n - n_1 \\)\n",
    "\n",
    "---\n",
    "\n",
    "#### ðŸ”¹ Posterior Derivation\n",
    "\n",
    "Using **Bayes' Theorem**:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(\\theta \\mid x_1, \\ldots, x_n) \\propto \\mathbb{P}(x_1, \\ldots, x_n \\mid \\theta) \\cdot \\mathbb{P}(\\theta)\n",
    "\\]\n",
    "\n",
    "Substitute the expressions for likelihood and prior:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(\\theta \\mid x_1, \\ldots, x_n) \\propto \\theta^{n_1}(1 - \\theta)^{n_0} \\cdot \\theta^{a - 1}(1 - \\theta)^{b - 1}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "= \\theta^{n_1 + a - 1} (1 - \\theta)^{n_0 + b - 1}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Conclusion:\n",
    "\n",
    "The posterior distribution is a **Beta distribution**:\n",
    "\n",
    "\\[\n",
    "\\mathbb{P}(\\theta \\mid x_1, \\ldots, x_n) \\propto \\theta^{n_1 + a - 1} (1 - \\theta)^{n_0 + b - 1}\n",
    "= \\text{Beta}(\\theta \\mid n_1 + a, n_0 + b)\n",
    "\\]\n",
    "\n",
    "This confirms the conjugacy: the **posterior** is in the **same family** as the **prior** (Beta).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam classifier\n",
    "\n",
    "The goal of this small project is to use a Naive bayes classifier to build a spam filter. To build our filter, we will use a dataset of 5,572 SMS messages put together by Tiago A. Almeida and JosÃ© MarÃ­a GÃ³mez Hidalgo. \n",
    "The dataset and the article describing the dataset are in the whiteboard directory together with this notebook. Of note, the SMS messages have already been processed for ease of use: all punctuation marks have been removed and the text has been transformed into lowercase. It is also common practice to remove any stop words such as `a`, `as`, `the` and to perform stemming (reduce words to their base form, such as stripping  the final `s*` in plural words, or the `*ing` from verbs (e.g., running becomes run)). For the sake of simplicity, we did not do that in this exercise.\n",
    "\n",
    "We will use a bag of word model:\n",
    " - We construct a corpus of the possible words $\\mathcal{D} = \\{w_1, \\ldots , w_d\\}$.\n",
    " - Each document is described by a vector of binary values $(x^{(1)}, \\ldots , x^{(d)})$ where $x^{(i)} = 1$ if $w_i$ occurs in the document and $0$ otherwise.\n",
    "\n",
    "The classification task is to predict for an SMS message if it is a _spam_ or a _ham_ (e.g. non-spam).\n",
    "\n",
    "Our data is thus $\\mathbf{x} = (x^{(1)},\\ldots, x^{(d)})$, $x^{(i)} \\in \\{0,1\\}$ and $y \\in \\{s,h\\}$\n",
    "We hypothesise that the values $x^{(i)}$ are drawn according to a Bernoulli distribution whose parameter depends on the class:\n",
    "$$\n",
    "\\mathbb{P}(x^{(i)} \\mid y = s) = \\theta_{i,s}^{x^{(i)}} \\cdot (1-\\theta_{i,s})^{1-x^{(i)}}\n",
    "$$\n",
    "and \n",
    "$$\n",
    "\\mathbb{P}(x^{(i)} \\mid y = h) = \\theta_{i,h}^{x^{(i)}} \\cdot (1-\\theta_{i,h})^{1-x^{(i)}}\n",
    "$$\n",
    "As we will use a naive Bayes classifier, the occurences of the different words are independent from each other.\n",
    "\\begin{align}\n",
    "\\mathbb{P}(\\mathbf{x} \\mid y = s) & = \\prod_{i=1}^{d} \\mathbb{P} (x^{(i)} \\mid y = s)\\\\\n",
    "  & =  \\prod_{i=1}^{d} \\theta_{i,s}^{x^{(i)}} \\cdot (1-\\theta_{i,s})^{1-x^{(i)}}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing library and loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "##Load the dataset (if in the same directory as the notebook)\n",
    "sms_data = np.loadtxt(\"C:/Users/TUÄžBA KABLAN/Desktop/ML-Spam Clasifier/02-2-SpamClassifier/SMSSpamCollection_cleaned.csv\", delimiter=\"\\t\", skiprows=1, dtype=str)\n",
    "\n",
    "## create test data set for checkpointing\n",
    "checkpoint_data = np.array([['spam', 'dear researcher submit manuscript money'], \n",
    "          ['ham','dear friend meet beer'],\n",
    "          ['ham', 'dear friend meet you']], dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['ham',\n",
       "        'go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat'],\n",
       "       ['ham', 'ok lar joking wif u oni'],\n",
       "       ['spam',\n",
       "        'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry question std txt rate t c s apply 08452810075over18 s']],\n",
       "      dtype='<U154')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Check the dataset\n",
    "sms_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total message number in data group: 5572\n",
      "Third messages's etiquet: spam\n",
      "Dividing the third message into the words : ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'question', 'std', 'txt', 'rate', 't', 'c', 's', 'apply', '08452810075over18', 's']\n"
     ]
    }
   ],
   "source": [
    "## Check the size of the dataset\n",
    "num_messages = sms_data.shape[0]\n",
    "print(f\"Total message number in data group: {num_messages}\")\n",
    "\n",
    "## Third message is a...\n",
    "print(\"Third messages's etiquet:\", sms_data[2][0])\n",
    "\n",
    "## Dividing the third message into words\n",
    "print(\"Dividing the third message into the words :\", sms_data[2][1].split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Construction of the Corpus\n",
    "\n",
    "Construct the corpus $\\mathcal{D}$ of all words in the dataset. The corpus will be represented as a hash table where each key is a unique word in the dataset and each value is the row index for that word. \n",
    " - How many unique words are there? \n",
    " - What are the 10 most common words (_e.g._ occuring in the most documents)?\n",
    " - Transform the set of messages in the form of a binary matrix of word occurrences.\n",
    " \n",
    " You can evaluate whether your implementation works using the checkpoint_data array. For this dataset the corpus could look as follows :\n",
    "\n",
    "`{'dear': 0, 'researcher': 1, 'submit': 2, 'manuscript': 3, 'money': 4, 'friend': 5, 'meet': 6, 'beer': 7, 'you': 8}`\n",
    "(of course you could have other index values for the words). \n",
    "\n",
    "The recoding of the checkpoint data will give you the following numpy array:\n",
    "\n",
    "```\n",
    "[[1. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 1. 1. 1. 0.]\n",
    " [1. 0. 0. 0. 0. 1. 1. 0. 1.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'go': 0, 'until': 0, 'jurong': 0, 'point': 0, 'crazy': 0, 'available': 0, 'only': 0, 'in': 0, 'bugis': 0, 'n': 0}\n"
     ]
    }
   ],
   "source": [
    "def construct_corpus(data):\n",
    "    \"\"\"\n",
    "    np.array[str, str] -> dict[str:int]\n",
    "    \n",
    "    from a 2D array of str, return a hash table\n",
    "    \"\"\"\n",
    "    corpus = {}  # Initialize an empty dictionary to store words and their row index\n",
    "    row_index = 0  # This will track the index of each message in the data\n",
    "\n",
    "    # Loop over the rows of the data\n",
    "    for label, message in data:\n",
    "        words = message.split()  # Split the message into words\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in corpus:  # If the word is not already in the corpus\n",
    "                corpus[word] = row_index  # Add the word with the current row index\n",
    "        \n",
    "        row_index += 1  # Move to the next message\n",
    "\n",
    "    return corpus  # Return the constructed corpus\n",
    "\n",
    "# Test the function with some data\n",
    "sms_data = np.array([['ham', 'go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat'],\n",
    "                     ['ham', 'ok lar joking wif u oni'],\n",
    "                     ['spam', 'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry question std txt rate t c s apply 08452810075over18 s']])\n",
    "\n",
    "# Construct the corpus from the data\n",
    "corpus = construct_corpus(sms_data)\n",
    "\n",
    "# Display the corpus (first 10 words for brevity)\n",
    "print({k: corpus[k] for k in list(corpus.keys())[:10]})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0.]\n",
      " [1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def recode_messages(data, corpus):\n",
    "    \"\"\"\n",
    "    np.array[str, str] * dict[str:int] -> np.array[int, int]\n",
    "    \n",
    "    returns the binary matrix encoding \n",
    "    \"\"\"\n",
    "    num_messages = data.shape[0]  # Number of messages\n",
    "    num_words = len(corpus)  # Number of unique words in the corpus\n",
    "    matrix = np.zeros((num_messages, num_words))  # Create a matrix with all zeros\n",
    "    \n",
    "    for i, (label, message) in enumerate(data):\n",
    "        words = message.split()  # Split the message into words\n",
    "        \n",
    "        for word in words:\n",
    "            if word in corpus:  # If the word is in the corpus\n",
    "                word_index = corpus[word]  # Get the index of the word from the corpus\n",
    "                matrix[i, word_index] = 1  # Mark the presence of the word in the binary matrix\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "D = construct_corpus(sms_data)  # Construct the corpus\n",
    "sms_matrix = recode_messages(sms_data, D)  # Convert messages into binary matrix\n",
    "\n",
    "print(sms_matrix)\n",
    "\n",
    "\n",
    "D = construct_corpus(sms_data)\n",
    "\n",
    "sms_matrix = recode_messages(sms_data, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Construct a training and a testing set and estimation of parameters\n",
    "\n",
    "\n",
    "To do the evaluation of the model afterward we will split the dataset randomly in two: \n",
    "- one dataset for training (80% of the messages) \n",
    "- one dataset for testing (20% of the messages).\n",
    "\n",
    "If you are familiar with it, you can use the `sklearn.model_selection` functions to construct the train and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2 messages\n",
      "Testing set size: 1 messages\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def train_test_split(X, Y, train_percentage=0.8):\n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "\n",
    "    number_examples = X.shape[0]\n",
    "    num_train = int(train_percentage * number_examples)\n",
    "  \n",
    "    assert X.shape[0] == Y.shape[0]\n",
    "    \n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    \n",
    "    number_examples = X.shape[0]\n",
    "    num_train = int(train_percentage * number_examples)\n",
    "    \n",
    "    X_train, Y_train = X[:num_train], Y[:num_train]\n",
    "    X_test, Y_test = X[num_train:], Y[num_train:]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "X = sms_matrix  \n",
    "Y = sms_data[:, 0] \n",
    "\n",
    "X_train, Y_train, X_test, Y_test = train_test_split(X, Y)\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} messages\")\n",
    "print(f\"Testing set size: {X_test.shape[0]} messages\")\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Estimation of the model parameters\n",
    "\n",
    "We will now estimate our model on the training set. This means estimating two types of parameters: the class prior, and the conditional word occurence probabilities.\n",
    "\n",
    "1.  Estimate the class prior $\\mathbb{P}(c) = \\mathbb{P}(y = c), (c = s, h)$\n",
    "2.  Using the results from section 1, compute the Maximum a posteriori estimator for the $d \\times 2$ matrix of parameters.\n",
    "$$\n",
    "\\Theta = \\left(\n",
    "\\begin{array}{cc}\n",
    "\\theta_{1,h}   & \\theta_{1,s} \\\\\n",
    "\\theta_{2,h} & \\theta_{2,s} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "\\theta_{d,h} & \\theta_{d,s} \\\\\n",
    "\\end{array}\n",
    "\\right).\n",
    "$$ You can use as conjugate prior a Beta(1, 1) distribution for instance (then $\\theta_{i,c} = \\frac{n_{i,c}+1}{N+2}$ where $n_{i,c}$ is the number of documents from the class $c$ where the word $w_i$ is present and $N$ is the total number of documents).\n",
    "\n",
    "When applied to the checkpoint data, your $\\Theta$ matrix should look like this:\n",
    "\n",
    "```\n",
    "# h    s\n",
    "[[0.75 0.66666667]  #'dear'\n",
    " [0.25 0.66666667]  #'researcher'\n",
    " [0.25 0.66666667]  #'submit'\n",
    " [0.25 0.66666667]  #'manuscript'\n",
    " [0.25 0.66666667]  #'money'\n",
    " [0.75 0.33333333]  #'friend'\n",
    " [0.75 0.33333333]  #'meet'\n",
    " [0.5  0.33333333]  #'beer'\n",
    " [0.5  0.33333333]] #'you'\n",
    "```\n",
    "\n",
    "3. Represent the fitted class conditional densities $(\\theta_{i,h})_{i \\in \\mathcal{D}}$ and $(\\theta_{i,s})_{i \\in \\mathcal{D}}$ like on the corresponding slide of the course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here for class priors\n",
    "    \n",
    "def estimate_proportions(data_matrix):\n",
    "    \"\"\"\n",
    "    estimate the matrix theta\n",
    "    \"\"\"\n",
    "    #your code here\n",
    "    \n",
    "##\n",
    "Dico = {'dear': 0, 'researcher':1, 'money': 2, 'friend': 3 }\n",
    "#sentence 1 dear friend money \n",
    "#sentence 2 dear researcher friend\n",
    "#sentence  friend money money\n",
    "datam = np.array([[1 0 1 1],[1 1 0 1 ],[0 0 1 1]], dtype = int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Message classification\n",
    "\n",
    "\n",
    "3. Classify the messages in the test set using the Maximum a posteriori rule, and evaluate the performance of the model by computing the True Positive Rate (also called Sensitivity) and the False Positive Rate (the same as 1-Specificity).\n",
    "\n",
    "4. The performance of the model above was obtained by using a classification threshold of $0.5$ on the posterior probability. In other words, if $\\mathbb{P}(y = s \\mid \\mathbb{x}) \\ge 0.5$ then the message is classified as spam. Draw a ROC curve for your classifier. Note that you have to consider multiple values of the threshold to draw the ROC curve. \n",
    "\n",
    "5. Why did we use the Maximum a posteriori estimator rather than the maximum likelihood one?\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Extension of the model \n",
    "\n",
    "One extension of the model is to consider a matrix of word counts instead of simply their presence/absence. \n",
    "The model will change in various ways in this case: \n",
    "   - We will count the total number of occurence in the spam or the ham set for each word.\n",
    "   - the words are now considered to occur independently along the sentence (independent Multinoulli). Thus, for a document with k words $\\mathbf{v}=(v_1,\\ldots, v_k)$\n",
    "    \\begin{align}\n",
    "    \\mathbb{P}(\\mathbf{v} \\mid y = s) \n",
    "  & =  \\prod_{t=1}^{k} p_{v_t}\n",
    "    \\end{align}\n",
    "       where $p_v$ is the probability to observe a word $v$\n",
    "       \n",
    "Note that with this new model we compute a product over the positions in the sentence while the bernoulli model did a product over all the words in the corpus.\n",
    "\n",
    "1. Implement the estimation of parameters for this model and the computation of the posterior class probabilities. This question can be interpreted in different ways, please explain your choices.\n",
    "2. Compare its accuracy and ROC curve with the previous model on a test set (*e.g.* go over section 3 and 4 again for this model).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
